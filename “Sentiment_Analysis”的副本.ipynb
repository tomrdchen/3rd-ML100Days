{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "“Sentiment Analysis”的副本",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomrdchen/3rd-ML100Days/blob/master/%E2%80%9CSentiment_Analysis%E2%80%9D%E7%9A%84%E5%89%AF%E6%9C%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwfoLyIvoj-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget \"https://www.dropbox.com/s/w03mdyw4kqimwgt/data.zip?dl=0\"\n",
        "!unzip data.zip\\?dl\\=0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8YITROzLPxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import torch\n",
        "import itertools\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, size=4, lines=[]):\n",
        "        assert size >= 4\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"PAD\", 3:\"UNK\"}\n",
        "        word2count = {}\n",
        "        for l in lines:\n",
        "            for word in l.split(' '):\n",
        "                if word not in word2count:\n",
        "                    word2count[word] = 1\n",
        "                else:\n",
        "                    word2count[word] += 1\n",
        "        word2count = list(word2count.items())\n",
        "        word2count.sort(key=lambda x: x[1], reverse=True)\n",
        "        size = min(size, len(word2count))\n",
        "        if len(lines):\n",
        "            print(\"{} words trimmed to {} words\".format(len(word2count), size))\n",
        "        for i in range(size-4):\n",
        "            self.index2word[i+4] = word2count[i][0]\n",
        "        self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "\n",
        "    def getIndex(self, word):\n",
        "        if word in self.word2index:\n",
        "            return self.word2index[word]\n",
        "        else:\n",
        "            return self.word2index[\"UNK\"]\n",
        "\n",
        "    def save2file(self, path):\n",
        "        with open(path, 'w') as f:\n",
        "            yaml.dump(self.index2word, f, default_flow_style=False, allow_unicode=True)\n",
        "\n",
        "    def load_file(self, path):\n",
        "        with open(path, 'r') as f:\n",
        "            self.index2word = yaml.load(f)\n",
        "            self.word2index = {v: k for k, v in self.index2word.items()}\n",
        "            self.size = len(self.index2word)\n",
        "\n",
        "class Twit_dataset(Dataset):\n",
        "    def __init__(self, csv_file, voc):\n",
        "        with open(csv_file) as f:\n",
        "            reader = csv.reader(f)\n",
        "            self.data = list(reader)\n",
        "        self.data = [(int(d[0]), [voc.getIndex(w) for w in d[1]]) for d in self.data]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    def pad(seqs, fillvalue=2):\n",
        "        tmp = list(itertools.zip_longest(*seqs, fillvalue=fillvalue))\n",
        "        return torch.LongTensor(tmp)\n",
        "    ys, xs = list(zip(*batch))\n",
        "    xs = pad(xs)\n",
        "    return torch.LongTensor(xs), torch.FloatTensor(ys)\n",
        "\n",
        "seqs = []\n",
        "size = 20000\n",
        "with open('data/train.csv') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for r in reader:\n",
        "        seqs.append(r[1])\n",
        "voc = Voc(size, seqs)\n",
        "train_dataset = Twit_dataset('data/train.csv', voc)\n",
        "valid_dataset = Twit_dataset('data/test.csv', voc)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=1, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=1, collate_fn=collate_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6UJogaKs2gS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class My_model(nn.Module):\n",
        "    def __init__(self, n_layers, voc_size, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(voc_size, 256)\n",
        "        self.rnn = nn.GRU(256, 256, n_layers, dropout=dropout)\n",
        "        self.fc = nn.Linear(256, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        emb = self.embedding(x)\n",
        "        output, hidden = self.rnn(emb)\n",
        "        output = self.fc(output[-1])\n",
        "        return torch.sigmoid(output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEikpXt02XJ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda'\n",
        "n_epoch = 5\n",
        "model = My_model(2, 20000).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "print_loss = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJfZLNYF2zGw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for e in range(n_epoch):\n",
        "    print('============= Epoch: {} ============='.format(e+1))\n",
        "    for idx, (x, y) in enumerate(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        o = model(x)\n",
        "        loss = criterion(o.squeeze(), y)\n",
        "        print_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (idx+1) % 100 == 0:\n",
        "            print('loss:', print_loss/100)\n",
        "            print_loss = 0.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}